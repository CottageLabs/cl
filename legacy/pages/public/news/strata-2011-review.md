#Strata 2011 Review
<br>

On February 1st 2011, I went on an adventure to Santa Clara in California - the heart of silicon valley, and the location of the first <a class="refs" name="strata" href="http://strataconf.com/strata2011">O'Reilly Strata conference</a>. The aim of my trip was to gauge the state of the art in a setting very different from our own.

#### Contents
 * [SETTING THE SCENE](#setting)
 * [STRATA KEYNOTES](#keynotes)
 * [TECHNICAL OVERVIEW](#technical)
 * [BUSINESS CASES](#business)
 * [REAL WORLD APPLICATIONS](#real)
 * [ETHICAL ISSUES](#ethics)
 * [PEOPLE AT STRATA](#people)
 * [APPLICATIONS IN OUR COMMUNITY](#applications)
 * [FUTURE DEVELOPMENTS](#future)
 * [SUMMARY](#summary)
 * [REFERENCES](#references)

<h2 id="setting">SETTING THE SCENE</h2>

The impact that freely available information has had on the learning community is truly profound, with tools like Wikipedia attracting huge audiences (over 365 million readers and growing) <a class="refs" name="wikipedia" href="http://en.wikipedia.org">Wikipedia</a>. We can now access more data than ever - but what do we do with it all? If we want to take full advantage of all this information do we really have the tools we need? And how do we develop these tools in the future?

The internet exerts an unprecedented equalizing force in bringing access to information to everyone on the planet. More information is available (and mainly for free) now than ever before, and yet it is becoming clear that access to information is not enough. The infrastructure to store and share data within sectors is a vital part of the ecosystem, and yet it is often treated as an afterthought. We need a radical change in the way we develop infrastructure in the higher education sector, to ensure that services consumed and funded by the public can do their job as efficiently as possible and at the best possible price.

The research agenda of a university department is closely matched to the skills and goals of the professors and lecturers working in that department. The topics researched in the History department will depend on the specific knowledge and expertise of the History professors at that university. If an external company were to offer to plan their research agenda for them, it would be met with obvious cynicism. And yet the critical tools that these departments rely on are often dismissed as a secondary priority â despite the fact that those very tools define the limits of our ability to explore and learn from the data space that is the foundation of all research.

The software that holds the millions of records used daily throughout the education sector are often not even considered; but going back even to the earliest times, it was the infrastructure â the ability to copy and reproduce text â that was the first step in developing academies of learning.  The tools that we use to analyse or work with data are an integral part of our day to day life - so much so that in most cases people are unaware that they are even using them. Searching for information has become such a common part of any job that little thought is given to the infrastructure required to generate the results.

In any example of a thriving educated community, the ability to freely determine the focus of learning has been vital; and within universities, the tools that underpin that research - the administrative infrastructure - remain vital to successful functioning. If we want to develop world class  education ecosystems we need to start thinking about the basis on which to build them. 

<h2 id="keynotes">STRATA KEYNOTES</h2>

Strata was an event dedicated to celebrating and advertising the latest tech trend: big data <a class="refs" name="bigdata" href="http://en.wikipedia.org/wiki/Big_data">Wikipedia description of big data</a>.

Visiting this oddly clean pseudo-industrial estate for the first time, I felt very much like a giant had taken the internet and painted it on the ground - such is the density of big business names that we have all become familiar with in recent years. I later learnt that much of the area is very new, built within the last few years, and that it is jokingly described as being like a âSim Cityâ <a class="refs" name="simcity" href="http://simcity.ea.com">Sim City computer game</a>. Clearly, there is wealth being generated here, and no sign of expansion abating - so there must be an exploitable resource somewhere in the vicinity...

Despite the disconcerting geography, the keynotes on days two and three echoed sentiments I have so recently discussed and advocated with friends and colleagues at home. All keynotes were recorded and are available online <a name="stratavids" class="refs" href="http://strataconf.com/strata2011/public/content/video">Strata conference keynote videos</a>, and I provide some paraphrased highlights:

 > a database is now uniquely valuable for the content in it rather than for the software it embodies; and people will fight to own these unique commodities. (Edd Dumbill - O'Reilly Media) <a name="oreilly" class="refs" href="http://oreilly.com">O'Reilly Media</a>

<span></span>

 > data mining has gone on for years; Thomson Reuters temper the extracted material by applying expertise to gain deep understanding. Big data continues to offer new opportunities, but there are also threats; privacy is an example of both; how do we make sure that individual privacy is not surrendered implicitly? privacy itself is an asset â exclusivity can be sold. (James Howell - Thomson Reuters) <a name="reuters" class="refs" href="http://thomsonreuters.com">Thomson Reuters</a>

<span></span>

 > when Microsoft say democratizing data it does not mean free data. It means offering the data for sale in an organised way. Microsoft works with data providers to clean and curate data, and offer it for sale to other developers across the world. (Zane Adam - Microsoft) <a name="microsoft" class="refs" href="http://microsoft.com">Microsoft</a>

From the preceding three samples we clearly see acceptance of the business case for datasets as a commodity, and we begin to see some of the politics behind big data as a business trend â these companies now claim democratisation and openness; and yet, at the same time, they are desperate to make us aware of how their longstanding expertise remains relevant, and of how they can own and sell products in this market.

 > Code has been commoditised, and now data is the market. But beware the mythology of big data. Just like the gold rush, most prospectors die hungry. Most people do not work in companies where data is the business â most companies make things. Therefore we must still overcome the difficulty of changing business processes. We have to work together on this â we cannot be lone prospectors. The real data revolution is in business structure and processes and how they use information. (Mark Madsen - Third Nature) <a name="thirdnature" class="refs" href="http://thirdnature.net">Third Nature</a>

<span></span>

 > Isolated datasets are not so interesting (in general) â but combining datasets can provide new answers. Instead of hoarding and curating data, we must open up and publish APIs. (Mike Olson - Cloudera <a name="cloudera" class="refs" href="http://cloudera.com">Cloudera</a>, Rod Smith - IBM <a name="ibm" class="refs" href="http://ibm.com">IBM</a>, Abhishek Metha - Tresata <a name="tresata" class="refs" href="http://www.tresata.com">Tresata</a>)

These samples illustrate a promising outlook not just regarding what can be commoditised and sold, but about the added benefit we can get from sharing and combining datasets; by working together with our data, we can learn so much more interesting things.

 > data scientists now self-identify! They manage and explore large datasets, and perform up-to-real time analyses. We are now only limited by education and imagination. We can use this for good â e.g. exposing tyranny by tracking the reduction in requests to the bit.ly service from Egypt during recent political unrest. (Hilary Mason - bit.ly) <a class="refs" name="bitly" href="http://bit.ly">bit.ly</a>
 
<span></span>

 > By sharing data a news organisation can enable more people to search for and find interesting things. This is more to do with open data than big data, although some of the datasets are big. In regard to a particular story, data can be useful and can give further insight, and generate related stories. However there are alternative examples â such as Wikileaks <a class="refs" name="wikileaks" href="http://wikileaks.ch">Wikileaks</a> â where the data is the story. This allows, for example, interactive online maps showing IED attacks over time; from this â and whether for good or for bad â we can find information that perhaps we had been told did not exist... (Simon Rogers - the Guardian) <a class="refs" name="guardian" href="http://www.guardian.co.uk">the Guardian</a>

<h2 id="technical">TECHNICAL OVERVIEW</h2>

As these keynotes introduce some complex technical topics, it is worth quickly reviewing the key technologies involved.

Over the last 30 years or so the key software product has been relational databases <a class="refs" name="rdbms" href="http://cacm.acm.org/magazines/1970/6/12368-a-relational-model-of-data-for-large-shared-data-banks/abstract">E.F. Codd, <em>A Relational Model of Data for Large Shared Data Banks</em>, 1970, Communications of the ACM 13 (6)</a>; many academic papers dealt solely with increasing database performance, and quite a few companies were built on these new discoveries. In fact the competitive advantage to different commercial relational databases was often defined in terms of millisecond differences on some particular task <a class="refs" name="rdbmsperf" href="http://www.tpc.org/">Transaction Processing Performance Council</a> â even though in reality these tiny differences were utterly irrelevant to the requirements of most of the people buying these products.

However in the early 21st century, the very thing that defined relational databases â their ability to store and quickly sort data in a clear, pre-defined way â became overly restrictive. I shall attempt to explain why:

 * A shopping list is a great way to remember what you need to buy.
 * If you are buying for quite a few people, you might start adding quantities.
 * When these quantities become the focus of attention, for example in totalling the number of items, it is useful to split up the types and the quantities, perhaps recording how many were in stock, and so on; this is when spreadsheets become useful.
 * It may then be useful to track businesses that stock particular items; but probably many items are stocked by one supplier, and some items may be stocked by multiple suppliers. To track all these relations between suppliers and products without having to update lots of address fields, for example, is when a relational database comes in handy.

This is the point all modern businesses find themselves at â their entire function can probably be distilled down to what is in their relational databases. But now, people use the world wide web to share data at an unprecedented level â and it is a mess; a vast ocean of information, no pre-defined structure, no organisation. This information did not grow up out of someone writing a shopping list, or by someone trying to run a business; instead, it is the totality of all general activities â no matter how inane.

This is big data. It is vast, and oddly shaped. We may still be interested in the products of various companies spread across different markets, but we do not know in advance what the pertinent information to collect is. In order to take advantage of this, we need new systems; they are probably non-relational, and they must be capable of handling and searching large amounts of data. Google, of course, are well known for offering search functionality across the ultimate dataset â the world wide web. They employ infrastructures and algorithms for handling and searching information:

 * Google File System (GFS) <a class="refs" name="gfs" href="http://labs.google.com/papers/gfs.html">S. Ghemawat, H. Gobioff, S. Leung, 2003, <em>The Google File System</em></a> provides redundantly distributed file storage across many-multiple commodity hard disks
 * MapReduce <a class="refs" name="mapreduce" href="http://labs.google.com/papers/mapreduce.html">J. Dean, S. Ghemawat, 2004, <em>MapReduce: Simplified Data Processing on Large Clusters </em></a> allows mapping across a very large dataset to identify a subset that meets particular parameters, discovering those subsets by performing the maps across distributed collections of data, passing the answers back, and performing reductions (and perhaps re-reductions) of the subset based on further criteria, such as summing them, to find an answer.
 * BigTable <a class="refs" name="bigtable" href="http://labs.google.com/papers/bigtable.html">F. Chang et al, 2006, <em>Bigtable: A Distributed Storage System for Structured Data</em></a> is an implementation of a database that works across the Google File System

Instead of keeping this to themselves, Google published papers explaining their techniques â and so other people started applying them. This led to a proliferation of open source <a class="refs" name="opensource" href="http://www.opensource.org/osd.html">Open source definition</a> tools to handle large datasets, which are available to anyone.

 * Eucalyptus <a class="refs" name="eucalyptus" href="http://open.eucalyptus.com/">Eucalyptus cloud platform</a> provides cloud (distributed computing) infrastructure
 * The Apache Hadoop project <a class="refs" name="hadoop" href="http://hadoop.apache.org/">Apache Hadoop project</a> includes various sub-projects that tackle particular aspects of providing open source reliable, scalable distributed computing
 * Apache Cassandra <a class="refs" name="cassandra" href="http://cassandra.apache.org">Apache Cassandra</a> provides a NoSQL <a class="refs" name="nosql" href="http://en.wikipedia.org/wiki/NoSQL">NoSQL definition</a> distributed database
 * Various companies offer software stacks built atop Hadoop, such as Karmasphere <a class="refs" name="karmasphere" href="http://www.karmasphere.com/">Karmasphere</a> and Cloudera
 * Other open source products such as CouchDB <a class="refs" name="couchdb" href="http://couchdb.apache.org">CouchDB</a> provide MapReduce functionality on non-relational databases, with multitudinous replication advantages.

With these technologies at hand, the only remaining technicality is that it is hard to know in advance the storage or processing requirements. Scaling can be a costly business. But that is exactly the sort of area where a large company can consolidate and offer economies of scale. Amazon is that company; behind their famous website that sells books (and now, every thing else a person could want) lies a very large amount of hardware, all of which can be accessed with one account at Amazon Web Services, as explained in Werner Vogels' keynote (paraphrased):

 > Big data is about 1) processing and managing large data sets, and 2) getting a competitive advantage from analysing all the data a business holds. But sometimes quality is far more important than quantity. Uncertainty as to the size of the valuable data set, or what processing must be done on it, make it difficult to predict storage and processing requirements .. so use scalable Amazon Web Services! (Werner Vogels - Amazon Web Services) <a class="refs" name="aws" href="http://aws.amazon.com">Amazon Web Services</a>

As the technicalities of point 1 are now clear, let us consider point 2.

<h2 id="business">BUSINESS CASES</h2>

With the technical capability to manage big datasets in place, business cases for exploiting opportunities with big data are appearing. Big data as a commodity does not actually necessarily rely on dealing with big datasets, but on selling the âbig dataâ product. When hardware became increasingly commoditised during the late 20th century, companies like Microsoft cashed in â and companies like IBM missed out. Now, with the tools and software required for doing big data available under open source licenses, software is being commoditised and data is the key product.

Perhaps, if a big dataset is lacking, the first step must be to build one. Lukas Biewald describes how his company utilise âCrowdsourcing and the democratisation of dataâ (paraphrased):

 > crowdsourcing enables us to generate useful data by distributing repetitive tasks to thousands of people, making it faster, shorter, less boring; just do it whilst waiting for the bus, or build it into something like the ESP game. This process has long been applied to problems like large scale arithmetic calculations; break them down into batches, have many unskilled people do simple sums, collect their answers and have more people sum those. Crowdflower adds value by offering confident estimates of accuracy by building in error handling; this is done by incorporating test cases that can be evaluated to check for people giving poor answers, and providing feedback loops to enable them to learn how to give better answers. (Lukas Biewald - Crowdflower) <a class="refs" name="crowdflower" href="http://crowdflower.com/">CrowdFlower</a>

These sorts of processes are known as mechanical turks <a class="refs" name="turks" href="http://en.wikipedia.org/wiki/The_Turk">The mechanical turk</a>, and various companies offer this service <a class="refs" name="amazonturk" href="https://www.mturk.com/mturk/welcome">Amazon Mechanical Turk</a>.

So with access to a big data source, it is time to apply some big data tools. This brings us back to the more familiar world of companies selling products because, although the underlying software may be open source, there is still a significant knowledge barrier to applying it; and if you do not know how, one way is to hire a company to do it for you.

I attended one particular session because I was interested in learning how Hadoop had developed since I last encountered it during my postgraduate studies, however it soon became clear I was actually being shown how to use the tools and services developed and sold by Karmasphere. Whilst I commend them on their development efforts, I must admit being disappointed; rather than learning new things, I was subject to the not-new experience of advertising. This demonstrates however that there is scope for selling training as well as services in this field, and the balance could be critical.

Most of the companies at Strata were advertising for data scientists, the definition of which is still somewhat fuzzy â not surprising for what is such a new profession. Maths, stats, programming, visualisation, and analysis skills are but a few desired qualities, the overall need being for people capable of accessing, aggregating, analysing, filtering and presenting data in meaningful â and profitable â ways.  It is not the tools themselves that are up for sale â as mentioned, they are mostly open source; these companies are not selling things, like databases or operating systems; they are applying expertise and selling services.

<em>Data science</em> in this sense relates to science-like activities â statistical analysis, complex computational problems â rather than scientific goals. Whilst scientific output has often been related to profit, these data scientists are employees, searching for ways to add value to products. Marshall Kirkpatrick, Simon Rogers and Jer Thorp demonstrated how big / open data is being used to support and extend traditional business output in âData journalism: Applied Interfacesâ (paraphrased):

 > Data extraction allows journalists to support and develop stories with more information â where people are, what they are saying, what is happening â these fundamentals can often be discovered far easier than in the past, but although it is easier, it is not new: Florence Nightingale used data journalism to show most soldiers died from preventable disease rather than combat, leading to improved hygiene and saved lives. However bigger data, open distributed access, and modern software enables real time visualisations, collaborative exploring and editing, and a wider source for more stories to develop. (Marshall Kirkpatrick - ReadWriteWeb <a class="refs" name="rww" href="http://www.readwriteweb.com/">ReadWriteWeb</a>, Simon Rogers - the Guardian, Jer Thorp - New York Times <a class="refs" name="nytimes" href="http://www.nytimes.com">the New York Times</a>)

The Guardian makes their investigative data available online, and there are many impressive examples available on their site <a class="refs" name="guardiandata" href="http://www.guardian.co.uk/data">the Guardian data store</a>. Similarly at the New York Times big data tools are being used to generate more interest in their stories; Jer Thorp demonstrated a tool <a class="refs" name="nytool" href="">New York Times twitter tool - unfortunately the projector failed and we could not read the link! If I (or anyone else) find it, I will update</a> that finds and visually displays tweets about their stories, built using various open source tools such as Processing <a class="refs" name="processing" href="http://processing.org/">Processing programming language</a>.

Further speculation as to how to apply this new technology continued in the real world applications seminars.

<h2 id="real">REAL WORLD APPLICATIONS</h2>

#### Healthcare and Medicine
Jim Golden (Accenture) <a class="refs" name="accenture" href="http://www.accenture.com">Accenture</a>, Carol McCall (Tenzing Health) <a class="refs" name="tenzing" href="http://tenzinghealth.com/">Tenzing Health</a>, Indu Subaiya (Health 2.0) <a class="refs" name="health2.0" href="http://www.health2con.com/">Health 2.0</a> and David van Sickle (Asthmapolis) <a class="refs" name="asthmapolis" href="http://asthmapolis.com/">Asthmapolis</a> raised some very interesting points in the Healthcare and Medicine panel discussion.

 * The annual cost of US healthcare is $3 trillion
 * 98.5% of patient records are still paper
 * 770000 people die yearly in the US from adverse drug reactions
 * Which could be solved by analysing what people take what drugs, for example
 * This could save an estimated 20%

Accenture has many clients that are interested in performing better clinical trials, getting the right people into the trial, finding out the efficacy of a particular drug. This evidence should drive healthcare reform. 

However Carol McCall points out that the healthcare business is actually the sickness business â it may be better for business for people to be sick â¦ Tenzing Health want to ensure that is not the way it is; instead, improved data should be used to keep people healthy and out of hospital. As an interesting example, her team of computational analysts developed a method for predicting end of life â¦ if a patient is likely to die, they probably do not want to be in hospital when it happens. But how to know which patients to send home? By analysing large datasets we can reliably estimate which patients are likely to fall into this category. 

Carol also gave a keynote talk in which she asked âcan big data fix healthcare?â

 * In five years, the cost of things people buy has increase five-fold, but healthcare increased 19-fold
 * the business case benefits from sickness, not health
 * there is documentary evidence of supply induced demand
 * all the information infrastructure revolves around getting paid, not improving health

Wisely, Carol describes the three key problems not as technical ones, but as follows:

 1. never underestimate resistance to change (people make huge amounts of money here)
 2. pick good problems (something where you can feasibly make a difference)
 3. data will suck -accept it and move on (it is designed for getting paid, not for getting well)

Health 2.0 hope to see patient blogs and drug user group information feed back into the healthcare system. The U.S. government are spending $20 billion on moving healthcare records from paper â but they have very sensibly attached a caveat to the funding, that developments must generate meaningful use; this means that rather than just replacing paper with PCs, new ways to work based on electronic data must be developed too â compiling easily accessible patient visit summaries, for example. Health related datasets will soon be made available for free to aid these developments.

Asthmapolis have a tiny data problem â there is virtually no healthcare data about asthma. Yet asthma sufferers are fairly unique in that they carry inhalers that they use when they have attacks â¦ so by tracking when and where asthma inhalers are used, data analysis could be used to better manage patient symptons, drastically reducing treatment costs. David van Sickle solved this problem â by simply attaching some basic electronics onto inhalers!

#### Education and Government
The Education and Government talk saw a lower turnout than Healthcare and medicine, perhaps as it is not so closely related to industry as healthcare is in the U.S. However given my bias towards higher education and research, I found that it was Steve Midgley of the U.S. Department for Education introducing us to the Learning Registry project <a class="refs" name="learningregistry" href="http://www.learningregistry.org/">The Learning Registry</a> that was the most exciting part of Strata:

 > We live in a fragmented world with an abundance of resources. How do we combine the right stuff to learn what we need? Wikipedia is a great place to start, but where do we go from there? How do we find and identify other valuable resources? For example, there must be course information existing in many places already, so why not just connect it up â find and deliver the right content to the right places / people, and contextualise that content with expertise, making a resource more useful.

 > What we do now does not cut it â buying 3000 biology textbooks and pushing them out the back of the truck and into the school is not the way of the future. Neither is building more closed-community âcustom portalsâ. How do we get to all the useful data that is already out there and make it findable? Being able to learn any time is a good thing, and we must support it by enabling a learning layer on the web. Innovate the infrastructure, not the interface. We need linked data and context â metadata in particular application contexts is not enough. 

 > In the education sector, we should be better able to share this sort of information because there is (or should be) less commercial restriction. Unfortunately, the lack of opportunity to make money means there are less people building for it. Learningregistry.org are working to overcome this by building common APIs, using tools such as CouchDB.

 > (Steve Midgley - U.S. Department for Education) <a class="refs" name="usdepted" href="http://www.ed.gov/">U.S. Department of Education</a>

Steve also quoted President Barack Obama as saying that âover the next ten years over half of all new jobs will require education beyond that provided at high schoolâ; the Learning Registry is striving to enable the greater need for access to education that this will entail, and it is all the more evidence why we must ensure in the UK that we meet these challenges too. However, even once we have the technology and the business case, it is worth considering some ethical concerns.

<h2 id="ethics">ETHICAL ISSUES</h2>

As with every other application of technology, big data raises ethical concerns. Judd Valesky (Gnip) <a class="refs" name="gnip" href="http://gnip.com/">Gnip</a>, Tim OâReilly (OâReilly Media), Dylan Field (Brown University) <a class="refs" name="brown" href="http://www.brown.edu/">Brown University</a> and Lucian Lita (BlueKai) <a class="refs" name="bluekai" href="http://www.bluekai.com">BlueKai</a> reviewed some of the common issues, and I will summarise them.

#### a different approach means different issues
 * The difference between traditional data warehouses and big data is that warehouses hold what we know, but big data holds stuff that we search in order to learn new things.
 * Whilst this entails that collecting everything may be useful, data about what people do (behaviours) is better than data about what people say.
 * This has good and bad consequences; can we get direct representation in digital democracy? will lack of record be (more) suspicious (or incriminating)?

#### individuals are not used to caring about their data shadow
 * People want free access to content, and as consumers we want discounts, but want control of information businesses hold.
 * People often sign many rights away implicitly in order to get the things they want. 
 * the dubious âcredit recordâ is a well known example; having never had credit, one still has a credit record, and it is negative! And the individual has no real control over their own record â it is ephemeral.

#### opt-out may not be an option
 * Whilst it may be ideal if everyone could control access to and maintain their own data, it is a bit like being invisible â we don't know how to do it.
 * Although we have the right not to engage, which would equate to being invisible, that may not remain possible for long â for example once government or medical records become digital by default, we may have no choice but to have an online profile of some sort.

#### without oversight, there is potential for exploitation
 * Businesses attempt to profit by following our tracks in cyberspace â the âdata exhaustâ; do we have a right to own it, or is it just a by-product of our online actions?
 * If we do not know who owns these datasets, it is hard to define the point at which curation becomes creation. If two datasets were combined, how would the revenue be split?
 * If a company have a terms of service agreement longer than a few lines, they are probably doing something wrong.

#### changing perceptions is non-trivial
 * Perhaps we must make people care about their digital shadow by developing an ecosystem that has the right penalties and benefits. 
 * Great effort goes into keeping medical records (for example) secret â usually to prevent adverse harm from insurance companies penalising us based on our medical history. Perhaps it would be better to change that practice instead of going to all the effort of keeping secrets.

Technology does not just speed up the things that we do; technology can also drive process change â for better or worse. These processes begin <em>before</em> ethical concerns are resolved; what is right will in some sense be decided by what is <em>done</em>. Under-representation ensures disenfranchisement by default, therefore we must have an active stake in the processes taking place both within and around our own community. As a guide to potential solutions, we can analyse the dispositions of people involved in the business and other communities with which we interact.

<h2 id="people">PEOPLE AT STRATA</h2>

Many critical pieces of industrial software have long been open source, yet closed source solutions tend to be considered valuable because they have a price tag, whereas open source solutions may be considered lower quality because they do not. This is evidently not true, now more than ever before; we must ensure that our community is aware of that, and that our procurement strategies take this into account. This is not an argument for open source on an ethical basis, although there are many such arguments also; rather I wish to show that to succeed in mitigating risk by developing or procuring the best solutions for our community, it is now very likely that the most suitable option is not the most expensive option â at least in terms of traditional expense criteria, such as licensing.

Well established businesses accepting this as a given â in a discussion with David Segleau of Oracle <a class="refs" name="oracle" href="http://www.oracle.com">Oracle</a>, for example, he explained to me that he was attending Strata specifically to learn how his team can adjust their product to better fit the latest use cases â the product in question being the famous open source database Berkeley DB <a class="refs" name="berkeleydb" href="http://www.oracle.com/technetwork/database/berkeleydb/overview/index.html">Berkeley DB</a>.

Similarly, EnterpriseDB <a class="refs" name="enterprisedb" href="http://www.enterprisedb.com">EnterpriseDB</a> found their business model on offering the open source PostgreSQL <a class="refs" name="postgresql" href="http://www.postgresql.org/">PostgreSQL</a> in a very traditional market space (relational databases). Rather than selling the product, EnterpriseDB add value and generate revenue by supporting process change, implementation, training and other ancillaries.

Sal Uryasev is a data scientist at LinkedIn <a class="refs" name="linkedin" href="http://www.linkedin.com">LinkedIn</a>. I discussed with him the latest service offered by his company â LinkedIn Skills <a class="refs" name="linkedinskills" href="http://ww.linkedin.com/skills">LinkedIn skills search service</a>. He was very happy to openly describe the software stack on which they had built the service (open source, again â including functionality from tools such as Hadoop and Gephi <a class="refs" name="gephi" href="http://gephi.org/">Gephi open graph vis platform</a>; there was no clandestine industrial espionage obfuscation going on here â I could ask whatever I wanted. So what is it that enables a group of people to offer this service? As Sal explained, and as his colleague DJ Patil described during a keynote presentation, LinkedIn are succeeding here because they recognise the value of data. It is not the technology that has made the difference, but the attitude of the people involved; their teams focus on developing services that find and expose useful information, not on developing software solutions.

There are also opportunities to learn from people situated in similarly less profit-driven communities. JJ Toothman is Web Strategist at NASA Ames Research Center <a class="refs" name="nasaames" href="http://www.nasa.gov/centers/ames/home/index.html">NASA Ames research center</a>; I caught up with him after his personal presentation at Strata of âData as Artâ <a class="refs" name="dataasart" href="http://nasawebdude.com/2011/02/data-as-art-presentation-at-strata-conference/">J.J. Toothman, <em>Data as art</em>, presentation Feb 2011</a>. During our discussion I learnt that NASA hopes to ensure the bulk of their available funding is spent on main aims rather than infrastructure. The key to achieving this is through sharing information within their distributed community â where one group is doing something useful, spreading that method to other groups. They aim to use the web and data as openly as possible to strengthen and enable their work rather than just acting as an advertisement for it, and are looking into the best ways of doing so, including the possible applications of Linked Data <a class="refs" name="linkeddatapres" href="http://www.slideshare.net/juansequeda/introduction-to-linked-data-2341398"><em>Introduction to linked data</em>, presentation, International Semantic Web Conference 2009</a> <a class="refs" name="linkeddataarticle" href="http://semanticweb.com/linked-data-an-introduction_b17148">J. Sequeda, <em>Introduction to: Linked data</em>, 2010, semanticweb.com</a> <a class="refs" name="linkeddatahs" href="http://linkeddata.jiscpress.org/">P. Miller, <em>Linked data horizon scan</em>, JISC</a>.

<h2 id="applications">APPLICATIONS IN OUR COMMUNITY</h2>

We are a community that provides and consumes a public service - therefore ideally capable of taking a less commercial approach to data and service sharing and community development.

There are opportunities to take advantage of the services that businesses offer without compromising sustainability. Amazon, for example, provide a good model for accessing and managing scalable storage and processing infrastructure, and any education-based project can submit a proposal to receive free AWS services <a class="refs" name="awsedu" href="http://aws.amazon.com/education">Amazon Web Services education offers</a>; they also maintain a list of some publicly available datasets <a class="refs" name="awspubs" href="http://aws.amazon.com/publicdatasets">Amazon Web Services public data sets</a>. The open source release of tools by companies such as Google has already been noted as a benefit (see also Fusion tables <a class="refs" name="fusion" href="http://www.google.com/fusiontables">Google Fusion tables</a> and Refine <a class="refs" name="refine" href="http://code.google.com/p/google-refine/">Google Refine</a>, amongst many others).

Of course, any technology incurs ancillary costs â development, maintenance, integration, process change and training, must all be considered. However these are demonstrably unavoidable, regardless of the solution on offer; simply buying in a licensed solution in the hope that we can avoid all these complications is a false economy in a sector where we cannot overcome that expenditure by selling a product at an inflated price; in higher education, we are not in the business of selling, but of providing a service with a far less tangible output.

Whilst data science may now be an identifiable profession, it is the average producers and consumers of data that must learn a new way of understanding how we work with and share information. Ensuring our staff and community members are equipped to deal with data in this new age will save us from surrendering control of our data just when it is recognised as a valuable commodity; it would be unfortunate to overcome barriers such as archaic copyright laws only to find our own output remains inaccessible to us except by means of a service contract.

The Microsoft keynote talk presented an honest example of how that company intends to profit from data â by adding value through cleaning and curation processes, companies justify selling data sets. However we should not forget that this is somewhat contrary to the purpose of the world wide web â originally designed and developed in an academic institution - for easily and freely sharing data. Democratized data now appears to mean âdata that is available to anyone â for a priceâ, so âopenâ data may be something of a marketing ploy. The question is one of balance; where businesses offer to support communities by providing services that add value and generate reasonable return for their efforts, and can offer savings via economies of scale, we should certainly consider taking advantage of what is on offer; however, we must also maintain free access to an increasingly valuable community resource.

<h2 id="future">FUTURE DEVELOPMENTS</h2>

An advantage of the move toward data commoditisation is that it requires data to be easily and preferably openly available â via APIs <a class="refs" name="api" href="http://en.wikipedia.org/wiki/Application_programming_interface">Wikipedia description of Application Programming Interface</a> or data markets <a class="refs" name="datamarket" href="http://datamarket.com/">Datamarket</a> and data stores <a class="refs" name="datastores" href="http://ckan.net/">Comprehensive Knowledege Archive Network project, Open Knowledge Foundation</a> <a class="refs" name="datagov" href="http://data.gov.uk/">UK Government open datasets</a>, for example. Big data is about linking data; by aggregating vast quantities of small data sets, a larger corpus can be created and studied from a new perspective, and new subsets can be carved out for specific needs. We have many such small datasets, and as these become more open, we will have increasing opportunities to exploit the available tools and services.

Joseph Turian of metaoptimise.com <a class="refs" name="metaoptimize" href="http://metaoptimize.com/">metaoptimize</a> presented <em>New developments in large data techniques</em>, pointing out that big data gives some advantage, but eventually provides diminishing returns; and at that point, the only alternative is to use better algorithms. He claims that MapReduce approaches will work well for shallow datasets, but for more complex inter-related datasets we will need a graph based solution <a class="refs" name="graphdata" href="http://en.wikipedia.org/wiki/Graph_database">Description of graph-based databases</a>. Whilst the technicalities of this are complex, it shows that even now we can foresee the edge of usefulness to the latest buzz â big data is not unlimited, it is just bigger.

Big data is just one more step up from not-so-big data - scaling up is in itself a limited process. Alternatively, if linking information were present within the data, it would become even easier to find. Formal linked data <a class="refs" name="formallinkeddata" href="http://www.w3.org/DesignIssues/LinkedData.html">W3 Linked Data Design Issues</a> as specified by Professor Sir Tim Berners-Lee (credited with inventing the world wide web) is under consideration as a solution to various problems, and some companies are now focussing on developing and offering linked data solutions <a class="refs" name="talis" href="http://www.talis.com/platform/">Talis</a> <a class="refs" name="virtuoso" href="http://virtuoso.openlinksw.com">Openlink Virtuoso</a>.

The best advertisement for any university is the learning that happens there. In 2008/2009 the top three university website redesign projects each cost well over Â£200,000 <a class="refs" name="siteredesigns" href="http://www.telegraph.co.uk/education/universityeducation/8042168/Universities-spending-millions-on-websites-which-students-rate-as-inadequate.html">The Telegraph, <em>Universities spending millions on websites which students rate as inadequate</em></a> and for that sort of money, it is only reasonable to expect some functionality that caters for the increasingly distributed and interactive nature of our relationship with data. A university web presence should no longer simply supplement a University Prospectus â it should be the public access point to a dynamic tool for interacting with the output of the community it represents.

If this big data adventure sees our data sets opening up and becoming easier to manage, explore and integrate, then with the addition of inferential links within and between data, and presuming we develop and sustain the public right of access to information, we can enable a learning community on an unprecedented scale - we can make the atoms of education and research discoverable by and accessible to anyone in the world.

<h2 id="summary">SUMMARY</h2>

#### the evolution of intangible commodities
 * hardware was successfully commoditized - software became the valued artefact
 * software is increasingly commoditized - now data are the valuable artefacts
 * but linking data will fully commoditize it
 * knowledge and understanding will become the valued artefacts

#### how to capitalise 
 * data is worthless without context and understanding
 * an expert ability to disseminate knowledge and understanding will become an increasingly valuable service 
 * focus on offering such a service as efficiently as possible - <em>scalable</em> education infrastructure

#### things to consider
 * software is becoming cheap - licensing costs require strong justification
 * there are still ancillary costs to cover, and economies of scale are good
 * share resources wherever possible - doing so is no longer a technical or geographical problem
 * move beyond the competitive mentality - collaboration is easy, if we are brave enough to do it
 * restrictive licensing paradigms are archaic; if they are not beneficial, do not pay for them

#### confident attitude required
 * if data is less open, it is less useful - limiting access limits value; the right to data should be inalienable
 * educational institutions once controlled access to information, but that was a by-product of the act of <em>enabling</em> access to information; controls were operational constraints, not mission statements
 * we do not need to (and can no longer) rely on closed data; do not be scared to advocate for openness
 * when data is freely available, it is our skills in explaining, contextualising and disseminating that become valuable

<h2 id="references">REFERENCES</h2>



Original Title: Strata 2011 review
Original Author: mark
Tags: bigdata, conference, data, mark, researchdatamanagement, strata, news
Created: 2011-02-09 1155
Last Modified: 2013-03-02 1859
